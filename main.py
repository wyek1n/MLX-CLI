import typer
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt, IntPrompt
from rich.progress import Progress, TextColumn, BarColumn, DownloadColumn, TransferSpeedColumn, TimeRemainingColumn
from rich.logging import RichHandler
from rich import print
from typing import Optional, List, Any, Dict
import os
import subprocess
from enum import Enum
import logging
import sys
from time import sleep
from dotenv import load_dotenv
import json
import yaml
from pathlib import Path
import wandb
from datetime import datetime, timedelta
from threading import Thread
from queue import Queue
import signal
from contextlib import contextmanager
import re
import requests
import time
import math
import matplotlib.pyplot as plt
import pandas as pd
import psutil
import traceback
import numpy as np

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆ›å»ºlogsç›®å½•
LOGS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
os.makedirs(LOGS_DIR, exist_ok=True)

class ToggleableRichHandler(RichHandler):
    """å¯åˆ‡æ¢æ˜¾ç¤ºçš„ RichHandler"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.enabled = False

    def emit(self, record):
        if self.enabled:
            super().emit(record)

# é…ç½®æ—¥å¿—
show_logs = False  # æ§åˆ¶æ—¥å¿—æ˜¾ç¤º
log_file = os.path.join(LOGS_DIR, f"mlx-cli-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log")
rich_handler = ToggleableRichHandler(
    rich_tracebacks=True,
    show_time=False,
    show_path=False
)
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="[%X]",
    handlers=[
        rich_handler,
        logging.FileHandler(log_file)
    ]
)
log = logging.getLogger("mlx-cli")

class DownloadSource(str, Enum):
    MODELSCOPE = "modelscope"
    HUGGINGFACE = "huggingface"

# åˆå§‹åŒ–
app = typer.Typer(
    name="MLX CLI",
    help="MLXæ¡†æ¶æ¨¡å‹è®­ç»ƒå’Œæ¨ç†å·¥å…·",
    add_completion=False
)

console = Console()

# ç¯å¢ƒå˜é‡é…ç½®
MLX_HOME = os.getenv("MLX_HOME")  # MLXæœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„
BASE_MODEL_DIR = os.path.join(MLX_HOME, "model") if MLX_HOME else None  # æ¨¡å‹ç›®å½•
BASE_DATASET_DIR = os.path.join(MLX_HOME, "dataset") if MLX_HOME else None  # æ•°æ®é›†ç›®å½•
ADAPTER_DIR = os.path.join(MLX_HOME, "adapter") if MLX_HOME else None  # æƒé‡æ–‡ä»¶ç›®å½•

# ä»ç¯å¢ƒå˜é‡è·å– API tokens
MODELSCOPE_TOKEN = os.getenv("MODELSCOPE_TOKEN")
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

# æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
def check_env_vars():
    """æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡æ˜¯å¦å­˜åœ¨"""
    required_vars = {
        "MLX_HOME": MLX_HOME,  # æ·»åŠ  MLX_HOME æ£€æŸ¥
        "BASE_MODEL_DIR": BASE_MODEL_DIR,
        "BASE_DATASET_DIR": BASE_DATASET_DIR,
        "ADAPTER_DIR": ADAPTER_DIR,  # æ·»åŠ  ADAPTER_DIR æ£€æŸ¥
        "MODELSCOPE_TOKEN": MODELSCOPE_TOKEN,
        "HUGGINGFACE_TOKEN": HUGGINGFACE_TOKEN,
        "WANDB_API_KEY": os.getenv("WANDB_API_KEY"),
        "TELEGRAM_BOT_TOKEN": os.getenv("TELEGRAM_BOT_TOKEN"),
        "TELEGRAM_CHAT_ID": os.getenv("TELEGRAM_CHAT_ID")
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    
    if missing_vars:
        console.print("[yellow]è­¦å‘Š: ä»¥ä¸‹ç¯å¢ƒå˜é‡æœªè®¾ç½®:[/yellow]")
        for var in missing_vars:
            console.print(f"[yellow]- {var}[/yellow]")
        if "MLX_HOME" in missing_vars:
            console.print("[red]é”™è¯¯: MLXæœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„æœªè®¾ç½®[/red]")
            raise typer.Exit(1)
        else:
            console.print("[yellow]éƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™[/yellow]")

def show_header():
    """æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯"""
    console.print(Panel.fit(
        "[bold blue]æ¬¢è¿ä½¿ç”¨ MLX CLI å·¥å…·[/bold blue]\n"
        "ç‰ˆæœ¬: 0.1.1",
        title="MLX CLI",
        border_style="blue"
    ))

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    try:
        import mlx
        console.print("[green]âœ“[/green] MLX ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
        try:
            from mlx_lm import load
            console.print("[green]âœ“[/green] MLX-LM ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
        except ImportError:
            console.print("[red]âœ—[/red] è¯·å…ˆå®‰è£… MLX-LM åŒ…")
            console.print("[yellow]è¿è¡Œ: pip install mlx-lm[/yellow]")
            raise typer.Exit(1)
    except ImportError:
        console.print("[red]âœ—[/red] è¯·å…ˆå®‰è£… MLX æ¡†æ¶")
        raise typer.Exit(1)

def show_menu():
    """æ˜¾ç¤ºä¸»èœå•"""
    console.print("\nè¯·é€‰æ‹©æ“ä½œ:")
    console.print("[1] æ¨¡å‹ä¸‹è½½")
    console.print("[2] æ¨¡å‹å¯¹è¯")
    console.print("[3] æ•°æ®å‡†å¤‡")
    console.print("[4] æ¨¡å‹å¾®è°ƒ")
    console.print("[5] æ¨¡å‹åˆå¹¶")
    console.print(f"[6] {'éšè—' if show_logs else 'æ˜¾ç¤º'}æ—¥å¿—")  # æ ¹æ®å½“å‰çŠ¶æ€æ˜¾ç¤º
    console.print("[0] é€€å‡ºç¨‹åº")
    
    return IntPrompt.ask("\nè¯·è¾“å…¥é€‰é¡¹", choices=["0", "1", "2", "3", "4", "5", "6"])

def check_model_exists(model_dir: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨"""
    return os.path.exists(model_dir)

def confirm_overwrite(model_dir: str) -> bool:
    """ç¡®è®¤æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ¨¡å‹"""
    console.print(f"\n[yellow]è­¦å‘Š: æ¨¡å‹ç›®å½•å·²å­˜åœ¨: {model_dir}[/yellow]")
    return Prompt.ask("æ˜¯å¦è¦†ç›–?", choices=["y", "n"], default="n").lower() == "y"

def download_model():
    """æ¨¡å‹ä¸‹è½½åŠŸèƒ½"""
    console.print("\n[bold cyan]æ¨¡å‹ä¸‹è½½[/bold cyan]")
    
    # é€‰æ‹©ä¸‹è½½æº
    console.print("\nè¯·é€‰æ‹©ä¸‹è½½æº:")
    console.print("[1] ModelScope (é»˜è®¤)")
    console.print("[2] HuggingFace")
    console.print("[0] è¿”å›ä¸»èœå•")
    
    source_choice = IntPrompt.ask("è¯·é€‰æ‹©", choices=["0", "1", "2"], default="1")
    
    if source_choice == 0:
        return
    
    # è¾“å…¥æ¨¡å‹åç§°
    model_name = Prompt.ask("\nè¯·è¾“å…¥æ¨¡å‹åç§° (ä¾‹å¦‚: Qwen/Qwen2.5-0.5B-Instruct)")
    model_dir = os.path.join(BASE_MODEL_DIR, model_name.split('/')[-1])
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    if check_model_exists(model_dir):
        if not confirm_overwrite(model_dir):
            console.print("[yellow]å·²å–æ¶ˆä¸‹è½½[/yellow]")
            return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(BASE_MODEL_DIR, exist_ok=True)
    
    try:
        if source_choice == 1:
            download_from_modelscope(model_name)
        else:
            download_from_huggingface(model_name)
    except Exception as e:
        console.print(f"[red]ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}[/red]")

def prepare_data():
    """æ•°æ®å‡†å¤‡åŠŸèƒ½"""
    console.print("\n[bold cyan]æ•°æ®å‡†å¤‡[/bold cyan]")
    
    console.print("\nè¯·é€‰æ‹©æ“ä½œ:")
    console.print("[1] æ•°æ®é›†ä¸‹è½½")
    console.print("[2] æ•°æ®é›†é¢„è§ˆ")
    console.print("[3] æ•°æ®é›†è½¬æ¢")
    console.print("[4] æ•°æ®é›†åˆ†å‰²")
    console.print("[0] è¿”å›ä¸»èœå•")
    
    choice = IntPrompt.ask("\nè¯·è¾“å…¥é€‰é¡¹", choices=["0", "1", "2", "3", "4"])
    
    if choice == 0:
        return
    elif choice == 1:
        download_dataset()
    elif choice == 2:
        preview_dataset()
    elif choice == 3:
        convert_dataset()
    elif choice == 4:
        split_dataset()

def merge_model():
    """æ¨¡å‹åˆå¹¶åŠŸèƒ½"""
    console.print("\n[bold cyan]æ¨¡å‹åˆå¹¶[/bold cyan]")
    
    try:
        # æ‰«ææ¨¡å‹ç›®å½•
        models = []
        for item in os.listdir(BASE_MODEL_DIR):
            if os.path.isdir(os.path.join(BASE_MODEL_DIR, item)) and not item.startswith('.'):
                models.append(item)
        
        if not models:
            console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹[/yellow]")
            return
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
        console.print("\n[bold]å¯ç”¨æ¨¡å‹åˆ—è¡¨:[/bold]")
        for i, model in enumerate(models, 1):
            console.print(f"[{i}] {model}")
        console.print("[0] è¿”å›ä¸»èœå•")
        
        # é€‰æ‹©åŸºç¡€æ¨¡å‹
        model_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©åŸºç¡€æ¨¡å‹",
            choices=["0"] + [str(i) for i in range(1, len(models) + 1)]
        ))
        
        if model_choice == 0:
            return
            
        selected_model = models[model_choice - 1]
        model_path = os.path.join(BASE_MODEL_DIR, selected_model)
        
        # æ‰«æé€‚é…å™¨ç›®å½•
        adapter_dir = ADAPTER_DIR  # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„è·¯å¾„
        adapters = []
        for item in os.listdir(adapter_dir):
            if os.path.isdir(os.path.join(adapter_dir, item)) and not item.startswith('.'):
                adapters.append(item)
        
        if not adapters:
            console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æƒé‡æ–‡ä»¶[/yellow]")
            return
        
        # æ˜¾ç¤ºå¯ç”¨æƒé‡æ–‡ä»¶åˆ—è¡¨
        console.print("\n[bold]å¯ç”¨æƒé‡æ–‡ä»¶åˆ—è¡¨:[/bold]")
        for i, adapter in enumerate(adapters, 1):
            console.print(f"[{i}] {adapter}")
        
        # é€‰æ‹©æƒé‡æ–‡ä»¶
        adapter_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©æƒé‡æ–‡ä»¶",
            choices=[str(i) for i in range(1, len(adapters) + 1)]
        ))
        selected_adapter = adapters[adapter_choice - 1]
        adapter_path = os.path.join(adapter_dir, selected_adapter)
        
        # è¾“å…¥æ–°æ¨¡å‹åç§°
        suffix = Prompt.ask("è¯·è¾“å…¥æ–°æ¨¡å‹åç§°åç¼€")
        if not suffix:
            console.print("[red]é”™è¯¯: æ¨¡å‹åç§°åç¼€ä¸èƒ½ä¸ºç©º[/red]")
            return
        
        # æ„å»ºæ–°æ¨¡å‹è·¯å¾„
        new_model_name = f"{selected_model}_{suffix}"
        save_path = os.path.join(BASE_MODEL_DIR, new_model_name)
        
        # æ£€æŸ¥æ–°æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(save_path):
            if not confirm_overwrite(save_path):
                console.print("[yellow]å·²å–æ¶ˆåˆå¹¶[/yellow]")
                return
        
        # æ‰§è¡Œæ¨¡å‹åˆå¹¶
        console.print(f"\n[yellow]æ­£åœ¨åˆå¹¶æ¨¡å‹...[/yellow]")
        merge_cmd = [
            "python", "-m", "mlx_lm.fuse",
            "--model", model_path,
            "--adapter-path", adapter_path,
            "--save-path", save_path
        ]
        
        try:
            subprocess.check_call(merge_cmd)
            console.print(f"\n[green]æ¨¡å‹åˆå¹¶æˆåŠŸï¼[/green]")
            console.print(f"æ–°æ¨¡å‹ä¿å­˜åœ¨: {save_path}")
            
            # å‘é€é€šçŸ¥
            merge_message = (
                f"ğŸ”„ <b>æ¨¡å‹åˆå¹¶å®Œæˆ</b>\n\n"
                f"åŸºç¡€æ¨¡å‹: {selected_model}\n"
                f"æƒé‡æ–‡ä»¶: {selected_adapter}\n"
                f"æ–°æ¨¡å‹åç§°: {new_model_name}"
            )
            send_telegram_message(merge_message)
            
        except subprocess.CalledProcessError as e:
            console.print(f"[red]åˆå¹¶å¤±è´¥: {str(e)}[/red]")
        except Exception as e:
            console.print(f"[red]æ‰§è¡Œå‡ºé”™: {str(e)}[/red]")
            
    except Exception as e:
        console.print(f"[red]å‡ºç°é”™è¯¯: {str(e)}[/red]")

def evaluate_model():
    """æ¨¡å‹è¯„ä¼°åŠŸèƒ½"""
    console.print("\n[bold cyan]æ¨¡å‹è¯„ä¼°[/bold cyan]")
    
    try:
        # æ‰«ææ¨¡å‹ç›®å½•
        models = []
        for item in os.listdir(BASE_MODEL_DIR):
            if os.path.isdir(os.path.join(BASE_MODEL_DIR, item)) and not item.startswith('.'):
                models.append(item)
        
        if not models:
            console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹[/yellow]")
            return
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
        console.print("\n[bold]å¯ç”¨æ¨¡å‹åˆ—è¡¨:[/bold]")
        for i, model in enumerate(models, 1):
            console.print(f"[{i}] {model}")
        
        # é€‰æ‹©æ¨¡å‹
        model_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©æ¨¡å‹",
            choices=[str(i) for i in range(1, len(models) + 1)]
        ))
        selected_model = models[model_choice - 1]
        model_path = os.path.join(BASE_MODEL_DIR, selected_model)
        
        # æ‰«æé€‚é…å™¨ç›®å½•
        adapter_dir = "/Users/wyek1n/Downloads/MLX/adapter"
        adapters = []
        for item in os.listdir(adapter_dir):
            if os.path.isdir(os.path.join(adapter_dir, item)) and not item.startswith('.'):
                adapters.append(item)
        
        if not adapters:
            console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨é€‚é…å™¨[/yellow]")
            return
        
        # æ˜¾ç¤ºå¯ç”¨é€‚é…å™¨åˆ—è¡¨
        console.print("\n[bold]å¯ç”¨é€‚é…å™¨åˆ—è¡¨:[/bold]")
        for i, adapter in enumerate(adapters, 1):
            console.print(f"[{i}] {adapter}")
        
        # é€‰æ‹©é€‚é…å™¨
        adapter_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©é€‚é…å™¨",
            choices=[str(i) for i in range(1, len(adapters) + 1)]
        ))
        selected_adapter = adapters[adapter_choice - 1]
        adapter_path = os.path.join(adapter_dir, selected_adapter)
        
        # æ‰§è¡Œè¯„ä¼°
        console.print(f"\n[yellow]æ­£åœ¨è¯„ä¼°æ¨¡å‹...[/yellow]")
        test_cmd = [
            "python", "-m", "mlx_lm.lora",
            "--model", model_path,
            "--data", "/Users/wyek1n/Downloads/Code/MLX/MLX-CLI/lora/data",
            "--adapter-path", adapter_path,
            "--test"
        ]
        
        try:
            test_output = subprocess.check_output(test_cmd, universal_newlines=True)
            test_match = re.search(r'Test loss ([0-9.]+),\s*Test ppl ([0-9.]+)', test_output)
            if test_match:
                test_loss = float(test_match.group(1))
                test_ppl = float(test_match.group(2).rstrip('.'))
                
                # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                console.print(f"\n[green]è¯„ä¼°ç»“æœ:[/green]")
                console.print(f"æ¨¡å‹: {selected_model}")
                console.print(f"é€‚é…å™¨: {selected_adapter}")
                console.print(f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
                console.print(f"æµ‹è¯•é›†å›°æƒ‘åº¦: {test_ppl:.4f}")
                
                # å‘é€è¯„ä¼°ç»“æœé€šçŸ¥
                eval_message = (
                    f"ğŸ“Š <b>æ¨¡å‹è¯„ä¼°ç»“æœ</b>\n\n"
                    f"æ¨¡å‹: {selected_model}\n"
                    f"é€‚é…å™¨: {selected_adapter}\n"
                    f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}\n"
                    f"æµ‹è¯•é›†å›°æƒ‘åº¦: {test_ppl:.4f}"
                )
                send_telegram_message(eval_message)
                
            else:
                console.print("[red]æ— æ³•è§£æè¯„ä¼°ç»“æœ[/red]")
                console.print(f"åŸå§‹è¾“å‡º: {test_output}")
        except Exception as e:
            log.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
            console.print(f"[red]è¯„ä¼°å¤±è´¥: {str(e)}[/red]")
            
    except Exception as e:
        console.print(f"[red]å‡ºç°é”™è¯¯: {str(e)}[/red]")

def chat_with_model():
    """æ¨¡å‹å¯¹è¯åŠŸèƒ½"""
    try:
        log.info("=== å¼€å§‹æ¨¡å‹å¯¹è¯åŠŸèƒ½ ===")
        # æ£€æŸ¥æ¨¡å‹ç›®å½•
        models = []
        log.info("å¼€å§‹æ‰«ææ¨¡å‹ç›®å½•")
        try:
            for item in os.listdir(BASE_MODEL_DIR):
                if os.path.isdir(os.path.join(BASE_MODEL_DIR, item)):
                    # è¿‡æ»¤æ‰ç‰¹æ®Šç›®å½•
                    if not item.startswith('.') and item != 'venv':
                        log.debug(f"æ‰¾åˆ°æ¨¡å‹ç›®å½•: {item}")
                        models.append(item)
        except FileNotFoundError as e:
            log.error(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {BASE_MODEL_DIR}", exc_info=True)
            console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•[/red]")
            return
        
        if not models:
            log.warning("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹")
            console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹[/yellow]")
            return
        
        log.info(f"å…±æ‰¾åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹")
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
        console.print("\n[bold]å¯ç”¨æ¨¡å‹åˆ—è¡¨:[/bold]")
        for i, model in enumerate(models, 1):
            console.print(f"[{i}] {model}")
        console.print("[0] è¿”å›ä¸»èœå•")
        
        # é€‰æ‹©æ¨¡å‹
        try:
            model_choice = int(Prompt.ask(
                "è¯·é€‰æ‹©æ¨¡å‹",
                choices=["0"] + [str(i) for i in range(1, len(models) + 1)]
            ))
            
            if model_choice == 0:
                return
                
            log.info(f"ç”¨æˆ·é€‰æ‹©äº†æ¨¡å‹ {models[model_choice - 1]}")
            
            # æ¸…ç†å±å¹•
            os.system('cls' if os.name == 'nt' else 'clear')
            
            # æ˜¾ç¤ºå¯¹è¯ç•Œé¢æ ‡é¢˜
            console.print("\n[bold cyan]æ¨¡å‹å¯¹è¯[/bold cyan]")
            selected_model = models[model_choice - 1]
            model_path = os.path.join(BASE_MODEL_DIR, selected_model)
            
            log.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
            console.print(f"\n[yellow]æ­£åœ¨åŠ è½½æ¨¡å‹ {selected_model}...[/yellow]")
            console.print(f"æ¨¡å‹è·¯å¾„: {model_path}")
            
            # ç¡®ä¿mlx_lmå·²å®‰è£…
            try:
                log.debug("æ­£åœ¨å¯¼å…¥ mlx_lm")
                from mlx_lm import load, stream_generate
            except ImportError:
                log.error("mlx_lm åŒ…æœªå®‰è£…")
                console.print("[red]é”™è¯¯: è¯·å…ˆå®‰è£… mlx-lm åŒ…[/red]")
                console.print("[yellow]è¿è¡Œ: pip install mlx-lm[/yellow]")
                return

            # å°è¯•åŠ è½½æ¨¡å‹
            log.info("å¼€å§‹åˆå§‹åŒ–æ¨¡å‹")
            console.print("[yellow]æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...[/yellow]")
            log.debug("è°ƒç”¨ load() å‡½æ•°")
            try:
                model, tokenizer = load(model_path)
                log.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                log.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", exc_info=True)
                raise
            console.print("[green]æ¨¡å‹åŠ è½½æˆåŠŸï¼[/green]")
            
            log.debug("åˆå§‹åŒ– Chatbot ç±»")
            class Chatbot:
                def __init__(self, model, tokenizer):
                    self.model = model
                    self.tokenizer = tokenizer
                    self.messages = [{"role": "system", "content": "æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"}]
                    log.debug("Chatbot åˆå§‹åŒ–å®Œæˆ")

                def add_user_message(self, content):
                    self.messages.append({"role": "user", "content": content})

                def get_response(self):
                    chat_template = self.tokenizer.apply_chat_template(
                        self.messages,
                        add_generation_prompt=True,
                        tokenize=False
                    )
                    return stream_generate(
                        self.model,
                        self.tokenizer,
                        prompt=chat_template,
                        max_tokens=2048
                    )

                def chat(self, user_input):
                    log.debug(f"æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_input}")
                    answer = ""
                    self.add_user_message(user_input)
                    response = self.get_response()
                    for text in response:
                        text_content = text.text if hasattr(text, 'text') else str(text)
                        answer = answer + text_content
                        yield text_content
                    self.messages.append({"role": "assistant", "content": answer})
                    log.debug("å®Œæˆä¸€è½®å¯¹è¯")
            
            # åˆ›å»ºèŠå¤©æœºå™¨äººå®ä¾‹
            log.info("åˆ›å»º Chatbot å®ä¾‹")
            chatbot = Chatbot(model, tokenizer)
            
            console.print("\n[green]æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼[/green]")
            console.print("[yellow]æç¤º: è¾“å…¥ 'exit' æˆ– 'quit' ç»“æŸå¯¹è¯[/yellow]\n")
            
            while True:
                try:
                    user_input = Prompt.ask("\n[bold blue]User[/bold blue]")
                    if user_input.lower() in ['exit', 'quit']:
                        console.print("\n[yellow]ç»“æŸå¯¹è¯[/yellow]")
                        break
                        
                    console.print("\n[bold green]Assistant[/bold green]", end="")
                    response = chatbot.chat(user_input)
                    for text in response:
                        if text != "":
                            print(text, end="", flush=True)
                    console.print()
                except Exception as e:
                    log.exception("å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
                    console.print(f"\n[red]å¯¹è¯å‡ºé”™: {str(e)}[/red]")
                    continue
            
        except ValueError:
            log.error("ç”¨æˆ·è¾“å…¥äº†æ— æ•ˆçš„é€‰æ‹©")
            console.print("[red]æ— æ•ˆçš„é€‰æ‹©[/red]")
            return
            
    except Exception as e:
        log.exception("æ¨¡å‹åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
        console.print(f"[red]åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}[/red]")
        console.print("[yellow]è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´ä¸”æ ¼å¼æ­£ç¡®[/yellow]")
        console.print(f"[yellow]è¯¦ç»†é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}[/yellow]")
        return
    finally:
        log.info("=== ç»“æŸæ¨¡å‹å¯¹è¯åŠŸèƒ½ ===")

def download_from_modelscope(model_name: str) -> None:
    """ä»ModelScopeä¸‹è½½æ¨¡å‹"""
    model_dir = os.path.join(BASE_MODEL_DIR, model_name.split('/')[-1])
    cmd = [
        "modelscope", "download",
        "--model", model_name,
        "--local_dir", model_dir
    ]
    
    try:
        with Progress(
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            DownloadColumn(),
            TransferSpeedColumn(),
            TimeRemainingColumn(),
            console=console,
            transient=True  # è¿™å°†ä½¿è¿›åº¦æ¡åœ¨å®Œæˆåæ¶ˆå¤±
        ) as progress:
            task = progress.add_task(f"[yellow]ä» ModelScope ä¸‹è½½æ¨¡å‹: {model_name}", total=None)
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            progress.update(task, completed=100)
            
        console.print(f"\n[green]æ¨¡å‹å·²æˆåŠŸä¸‹è½½åˆ°: {model_dir}[/green]")
    except subprocess.CalledProcessError as e:
        console.print(f"\n[red]ä¸‹è½½å¤±è´¥: {str(e)}[/red]")

def download_from_huggingface(model_name: str) -> None:
    """ä»HuggingFaceä¸‹è½½æ¨¡å‹"""
    model_dir = os.path.join(BASE_MODEL_DIR, model_name.split('/')[-1])
    cmd = [
        "huggingface-cli", "download",
        model_name,
        "--local-dir", model_dir
    ]
    
    try:
        with Progress(
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            DownloadColumn(),
            TransferSpeedColumn(),
            TimeRemainingColumn(),
            console=console,
            transient=True  # è¿™å°†ä½¿è¿›åº¦æ¡åœ¨å®Œæˆåæ¶ˆå¤±
        ) as progress:
            task = progress.add_task(f"[yellow]ä» HuggingFace ä¸‹è½½æ¨¡å‹: {model_name}", total=None)
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            progress.update(task, completed=100)
            
        console.print(f"\n[green]æ¨¡å‹å·²æˆåŠŸä¸‹è½½åˆ°: {model_dir}[/green]")
    except subprocess.CalledProcessError as e:
        console.print(f"\n[red]ä¸‹è½½å¤±è´¥: {str(e)}[/red]")

def download_dataset():
    """æ•°æ®é›†ä¸‹è½½åŠŸèƒ½"""
    console.print("\n[bold cyan]æ•°æ®é›†ä¸‹è½½[/bold cyan]")
    
    # é€‰æ‹©ä¸‹è½½æº
    console.print("\nè¯·é€‰æ‹©ä¸‹è½½æº:")
    console.print("[1] ModelScope (é»˜è®¤)")
    console.print("[2] HuggingFace")
    source_choice = IntPrompt.ask("è¯·é€‰æ‹©", choices=["1", "2"], default="1")
    
    # è¾“å…¥æ•°æ®é›†åç§°
    dataset_name = Prompt.ask("\nè¯·è¾“å…¥æ•°æ®é›†åç§° (ä¾‹å¦‚: xiaofengalg/ShenNong_TCM_Dataset)")
    dataset_dir = os.path.join(BASE_DATASET_DIR, dataset_name.split('/')[-1])
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨
    if check_model_exists(dataset_dir):
        if not confirm_overwrite(dataset_dir):
            console.print("[yellow]å·²å–æ¶ˆä¸‹è½½[/yellow]")
            return
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(BASE_DATASET_DIR, exist_ok=True)
    
    try:
        if source_choice == 1:
            download_dataset_from_modelscope(dataset_name)
        else:
            download_dataset_from_huggingface(dataset_name)
    except Exception as e:
        console.print(f"[red]ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}[/red]")

def download_dataset_from_modelscope(dataset_name: str, max_retries: int = 3) -> None:
    """ä»ModelScopeä¸‹è½½æ•°æ®é›†"""
    dataset_dir = os.path.join(BASE_DATASET_DIR, dataset_name.split('/')[-1])
    cmd = [
        "modelscope",
        "--token", MODELSCOPE_TOKEN,
        "download",
        "--dataset", dataset_name,
        "--local_dir", dataset_dir
    ]
    
    errors: List[str] = []
    for attempt in range(max_retries):
        try:
            with Progress(
                TextColumn("[bold blue]{task.description}"),
                BarColumn(),
                "[progress.percentage]{task.percentage:>3.0f}%",
                DownloadColumn(),
                TransferSpeedColumn(),
                TimeRemainingColumn(),
                console=console,
                transient=True
            ) as progress:
                task = progress.add_task(
                    f"[yellow]ä» ModelScope ä¸‹è½½æ•°æ®é›†: {dataset_name} (å°è¯• {attempt + 1}/{max_retries})",
                    total=None
                )
                result = subprocess.run(cmd, check=True, capture_output=True, text=True)
                progress.update(task, completed=100)
                
            console.print(f"\n[green]æ•°æ®é›†å·²æˆåŠŸä¸‹è½½åˆ°: {dataset_dir}[/green]")
            return  # ä¸‹è½½æˆåŠŸï¼Œç›´æ¥è¿”å›
        except subprocess.CalledProcessError as e:
            error_msg = f"å°è¯• {attempt + 1}: {str(e)}"
            if e.stderr:
                error_msg += f"\né”™è¯¯è¾“å‡º: {e.stderr.decode('utf-8')}"
            errors.append(error_msg)
            
            if attempt < max_retries - 1:
                console.print(f"[yellow]ä¸‹è½½å¤±è´¥ï¼Œ{5 * (attempt + 1)}ç§’åé‡è¯•...[/yellow]")
                sleep(5 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
            else:
                console.print("[red]è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸‹è½½å¤±è´¥[/red]")
                for error in errors:
                    console.print(f"[red]{error}[/red]")

def download_dataset_from_huggingface(dataset_name: str, max_retries: int = 3) -> None:
    """ä»HuggingFaceä¸‹è½½æ•°æ®é›†"""
    dataset_dir = os.path.join(BASE_DATASET_DIR, dataset_name.split('/')[-1])
    cmd = [
        "huggingface-cli", "download",
        dataset_name,
        "--repo-type", "dataset",
        "--local-dir", dataset_dir,
        "--token", HUGGINGFACE_TOKEN
    ]
    
    errors: List[str] = []
    for attempt in range(max_retries):
        try:
            with Progress(
                TextColumn("[bold blue]{task.description}"),
                BarColumn(),
                "[progress.percentage]{task.percentage:>3.0f}%",
                DownloadColumn(),
                TransferSpeedColumn(),
                TimeRemainingColumn(),
                console=console,
                transient=True
            ) as progress:
                task = progress.add_task(
                    f"[yellow]ä» HuggingFace ä¸‹è½½æ•°æ®é›†: {dataset_name} (å°è¯• {attempt + 1}/{max_retries})",
                    total=None
                )
                result = subprocess.run(cmd, check=True, capture_output=True, text=True)
                progress.update(task, completed=100)
                
            console.print(f"\n[green]æ•°æ®é›†å·²æˆåŠŸä¸‹è½½åˆ°: {dataset_dir}[/green]")
            return  # ä¸‹è½½æˆåŠŸï¼Œç›´æ¥è¿”å›
        except subprocess.CalledProcessError as e:
            error_msg = f"å°è¯• {attempt + 1}: {str(e)}"
            if e.stderr:
                error_msg += f"\né”™è¯¯è¾“å‡º: {e.stderr.decode('utf-8')}"
            errors.append(error_msg)
            
            if attempt < max_retries - 1:
                console.print(f"[yellow]ä¸‹è½½å¤±è´¥ï¼Œ{5 * (attempt + 1)}ç§’åé‡è¯•...[/yellow]")
                sleep(5 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
            else:
                console.print("[red]è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸‹è½½å¤±è´¥[/red]")
                for error in errors:
                    console.print(f"[red]{error}[/red]")

def detect_file_format(file_path: str) -> str:
    """æ£€æµ‹æ–‡ä»¶æ ¼å¼
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        str: 'json' æˆ– 'jsonl'
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            first_line = f.readline().strip()
            second_line = f.readline().strip()
            
            # å¦‚æœç¬¬äºŒè¡Œå­˜åœ¨ä¸”æ˜¯æœ‰æ•ˆçš„JSONï¼Œè¯´æ˜æ˜¯JSONLæ ¼å¼
            if second_line and json.loads(first_line) and json.loads(second_line):
                return 'jsonl'
            
            # é‡æ–°æ‰“å¼€æ–‡ä»¶å°è¯•ä½œä¸ºå•ä¸ªJSONè¯»å–
            f.seek(0)
            json.load(f)
            return 'json'
    except json.JSONDecodeError:
        # å¦‚æœä½œä¸ºå•ä¸ªJSONè¯»å–å¤±è´¥ï¼Œå†æ¬¡å°è¯•æŒ‰è¡Œè¯»å–
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        json.loads(line.strip())
                return 'jsonl'
        except json.JSONDecodeError:
            raise ValueError("æ— æ³•è¯†åˆ«æ–‡ä»¶æ ¼å¼")
    except Exception as e:
        raise ValueError(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

def convert_to_jsonl(file_path: str) -> str:
    """å°†JSONæ–‡ä»¶è½¬æ¢ä¸ºJSONLæ ¼å¼
    
    Args:
        file_path: æºæ–‡ä»¶è·¯å¾„
        
    Returns:
        str: è½¬æ¢åçš„JSONLæ–‡ä»¶è·¯å¾„
    """
    jsonl_path = file_path.rsplit('.', 1)[0] + '.jsonl'
    
    try:
        # é¦–å…ˆæ£€æµ‹æ–‡ä»¶æ ¼å¼
        file_format = detect_file_format(file_path)
        
        # å¦‚æœå·²ç»æ˜¯JSONLæ ¼å¼ï¼Œåªéœ€è¦é‡å‘½å
        if file_format == 'jsonl':
            if not file_path.endswith('.jsonl'):
                os.rename(file_path, jsonl_path)
                console.print(f"[green]å·²å°† {os.path.basename(file_path)} é‡å‘½åä¸ºJSONLæ ¼å¼[/green]")
            return jsonl_path
        
        # å¦‚æœæ˜¯JSONæ ¼å¼ï¼Œè¿›è¡Œè½¬æ¢
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨
        if not isinstance(data, list):
            data = [data]
            
        # å†™å…¥JSONLæ–‡ä»¶
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        # åˆ é™¤åŸæ–‡ä»¶
        os.remove(file_path)
        console.print(f"[green]å·²å°† {os.path.basename(file_path)} è½¬æ¢ä¸ºJSONLæ ¼å¼[/green]")
        return jsonl_path
    except Exception as e:
        console.print(f"[red]è½¬æ¢æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}[/red]")
        if os.path.exists(jsonl_path) and jsonl_path != file_path:
            os.remove(jsonl_path)  # æ¸…ç†æœªå®Œæˆçš„è½¬æ¢æ–‡ä»¶
        return file_path  # è½¬æ¢å¤±è´¥æ—¶è¿”å›åŸæ–‡ä»¶è·¯å¾„

def preview_dataset():
    """æ•°æ®é›†é¢„è§ˆåŠŸèƒ½"""
    console.print("\n[bold cyan]æ•°æ®é›†é¢„è§ˆ[/bold cyan]")
    
    # æ£€æŸ¥æ•°æ®é›†ç›®å½•
    datasets = []
    try:
        for item in os.listdir(BASE_DATASET_DIR):
            if os.path.isdir(os.path.join(BASE_DATASET_DIR, item)):
                if not item.startswith('.'):
                    datasets.append(item)
    except FileNotFoundError:
        console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®é›†ç›®å½•[/red]")
        return
    
    if not datasets:
        console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ•°æ®é›†ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®é›†[/yellow]")
        return
    
    # æ˜¾ç¤ºå¯ç”¨æ•°æ®é›†åˆ—è¡¨
    console.print("\n[bold]å¯ç”¨æ•°æ®é›†åˆ—è¡¨:[/bold]")
    for i, dataset in enumerate(datasets, 1):
        console.print(f"[{i}] {dataset}")
    
    # é€‰æ‹©æ•°æ®é›†
    try:
        dataset_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©æ•°æ®é›†",
            choices=[str(i) for i in range(1, len(datasets) + 1)]
        ))
    except ValueError:
        console.print("[red]æ— æ•ˆçš„é€‰æ‹©[/red]")
        return

    selected_dataset = datasets[dataset_choice - 1]
    dataset_dir = os.path.join(BASE_DATASET_DIR, selected_dataset)
    
    # æŸ¥æ‰¾å¹¶è¯»å–æ•°æ®æ–‡ä»¶
    data_files = []
    file_sizes = {}
    for root, _, files in os.walk(dataset_dir):
        for file in files:
            if file.endswith(('.json', '.jsonl')):
                file_path = os.path.join(root, file)
                try:
                    file_size = os.path.getsize(file_path)
                    data_files.append(file_path)
                    file_sizes[file_path] = file_size
                except OSError as e:
                    console.print(f"[yellow]è­¦å‘Š: æ— æ³•è·å–æ–‡ä»¶å¤§å° {file}: {str(e)}[/yellow]")
    
    if not data_files:
        console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶[/red]")
        return
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åº
    sorted_files = sorted(data_files, key=lambda x: file_sizes[x], reverse=True)
    largest_file = sorted_files[0]
    
    # å¦‚æœæœ€å¤§æ–‡ä»¶ä¸æ˜¯JSONLæ ¼å¼ï¼Œè¿›è¡Œè½¬æ¢
    if not largest_file.endswith('.jsonl'):
        console.print("\n[yellow]æ£€æµ‹åˆ°éJSONLæ ¼å¼æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢...[/yellow]")
        largest_file = convert_to_jsonl(largest_file)
        # æ›´æ–°æ–‡ä»¶å¤§å°ä¿¡æ¯
        file_sizes[largest_file] = os.path.getsize(largest_file)
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    console.print("\n[bold]æ‰¾åˆ°ä»¥ä¸‹æ•°æ®æ–‡ä»¶:[/bold]")
    for file in sorted_files:
        if file == largest_file or file.endswith('.jsonl'):  # åªæ˜¾ç¤ºJSONLæ–‡ä»¶
            size_mb = file_sizes[file] / (1024 * 1024)
            is_largest = file == largest_file
            console.print(
                f"{'[green]â†’[/green] ' if is_largest else '  '}"
                f"{os.path.basename(file)} "
                f"({size_mb:.2f} MB)"
                f"{' [yellow](å°†ä½¿ç”¨æ­¤æ–‡ä»¶)[/yellow]' if is_largest else ''}"
            )
    
    # è¯»å–æ•°æ®
    data = []
    try:
        with open(largest_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:  # è·³è¿‡ç©ºè¡Œ
                    data.append(json.loads(line))
    except Exception as e:
        console.print(f"[red]è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}[/red]")
        console.print("[yellow]æç¤º: è¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„ JSONL æ ¼å¼[/yellow]")
        return
    
    if not data:
        console.print("[red]é”™è¯¯: æ•°æ®é›†ä¸ºç©º[/red]")
        return
    
    # æ˜¾ç¤ºæ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    console.print(f"\n[bold]æ•°æ®é›†ä¿¡æ¯:[/bold]")
    console.print(f"æ€»è®°å½•æ•°: {len(data)}")
    console.print(f"æ–‡ä»¶å¤§å°: {file_sizes[largest_file] / (1024 * 1024):.2f} MB")
    
    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
    if not Prompt.ask("\næ˜¯å¦å¼€å§‹é¢„è§ˆ?", choices=["y", "n"], default="y").lower() == "y":
        return
    
    # é¢„è§ˆæ•°æ®
    page_size = 10
    current_page = 0
    total_pages = (len(data) + page_size - 1) // page_size
    
    # å¼€å§‹é¢„è§ˆå‰æ¸…å±
    console.clear()
    console.print("\n[bold cyan]æ•°æ®é›†é¢„è§ˆ[/bold cyan]")
    console.print("[yellow]æŒ‰ Q é€€å‡ºï¼ŒW ä¸Šä¸€é¡µï¼ŒE ä¸‹ä¸€é¡µ[/yellow]")
    
    while True:
        console.clear()
        console.print(f"\n[bold]æ•°æ®é›†: {selected_dataset}[/bold]")
        console.print(f"[bold]é¡µç : {current_page + 1}/{total_pages}[/bold]")
        
        start_idx = current_page * page_size
        end_idx = min(start_idx + page_size, len(data))
        
        for i in range(start_idx, end_idx):
            console.print("\n" + "â”€" * 80)
            console.print(f"[bold cyan]è®°å½• {i + 1}[/bold cyan]")
            console.print(json.dumps(data[i], ensure_ascii=False, indent=2))
        
        console.print("\n" + "â”€" * 80)
        console.print("\n[yellow]Q: é€€å‡º | W: ä¸Šä¸€é¡µ | E: ä¸‹ä¸€é¡µ[/yellow]")
        
        # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©åˆé€‚çš„è¾“å…¥æ–¹æ³•
        if os.name == 'nt':
            import msvcrt
            key = msvcrt.getch().decode().lower()
        else:
            import tty
            import termios
            fd = sys.stdin.fileno()
            old_settings = termios.tcgetattr(fd)
            try:
                tty.setraw(sys.stdin.fileno())
                key = sys.stdin.read(1).lower()
            finally:
                termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
        
        if key == 'q':
            console.clear()  # é€€å‡ºé¢„è§ˆæ—¶æ¸…å±
            break
        elif key == 'w' and current_page > 0:
            current_page -= 1
        elif key == 'e' and current_page < total_pages - 1:
            current_page += 1

class MLXDataConverter:
    """MLXæ•°æ®é›†è½¬æ¢å™¨"""
    # å®šä¹‰å­—æ®µæ˜ å°„
    INPUT_FIELDS = ["question", "query", "instruction", "prompt", "task"]  # ä¸»è¾“å…¥
    CONTEXT_FIELDS = ["context", "input", "metadata", "example", "evidence", "schema"]  # ä¸Šä¸‹æ–‡
    OUTPUT_FIELDS = ["answer", "response", "output", "result", "solution", "completion"]  # è¾“å‡º

    def __init__(self, input_data: Any, target_format: str = None, system_message: str = None):
        self.input_data = input_data
        self.target_format = target_format
        self.system_message = system_message
        self.format_prefixes = {
            "chat": {
                "context": "",
                "input": ""
            },
            "completions": {
                "context": "",
                "input": ""
            },
            "text": {
                "context": "",
                "input": ""
            }
        }

    def detect_format(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹è¾“å…¥æ•°æ®ç‰¹å¾å¹¶æ¨æ–­ç›®æ ‡æ ¼å¼"""
        if isinstance(self.input_data, list) and all("role" in item for item in self.input_data):
            return "chat"
        elif isinstance(self.input_data, dict):
            if any(f in self.input_data for f in self.INPUT_FIELDS):
                return "completions"
        return "text"

    def _get_prefixes(self) -> Dict[str, str]:
        """è·å–å½“å‰æ ¼å¼çš„å‰ç¼€é…ç½®"""
        format_to_use = self.target_format or self.detect_format()
        return self.format_prefixes.get(format_to_use, {"context": "", "input": ""})

    def convert_to_chat(self) -> Dict[str, list]:
        """è½¬æ¢ä¸ºchatæ ¼å¼"""
        messages = []
        
        if isinstance(self.input_data, dict):
            input_text = next((self.input_data[f] for f in self.INPUT_FIELDS if f in self.input_data), "")
            context_text = next((self.input_data[f] for f in self.CONTEXT_FIELDS if f in self.input_data), "")
            completion = next((self.input_data[f] for f in self.OUTPUT_FIELDS if f in self.input_data), "")
            
            full_prompt = ""
            prefixes = self._get_prefixes()
            if context_text:
                full_prompt += f"{prefixes['context']}{context_text}\n\n"
            if input_text:
                full_prompt += f"{prefixes['input']}{input_text}"
            
            if full_prompt:
                messages = [
                    {"role": "user", "content": full_prompt.strip()},
                    {"role": "assistant", "content": completion}
                ]
        else:
            messages = [{"role": "user", "content": str(self.input_data)}]
            
        # æ·»åŠ system message
        if self.system_message is not None:
            messages.insert(0, {
                "role": "system",
                "content": self.system_message
            })
            
        return {"messages": messages}

    def convert_to_completions(self) -> Dict[str, str]:
        """è½¬æ¢ä¸ºcompletionsæ ¼å¼"""
        if isinstance(self.input_data, dict):
            input_text = next((self.input_data[f] for f in self.INPUT_FIELDS if f in self.input_data), "")
            context_text = next((self.input_data[f] for f in self.CONTEXT_FIELDS if f in self.input_data), "")
            output_text = next((self.input_data[f] for f in self.OUTPUT_FIELDS if f in self.input_data), "")
            
            full_prompt = ""
            if context_text:
                full_prompt += f"{context_text}\n\n"
            if input_text:
                full_prompt += input_text
            
            # æ·»åŠ system messageåˆ°promptå¼€å¤´
            if self.system_message:
                full_prompt = f"{self.system_message}\n{full_prompt}"
            
            return {"prompt": full_prompt.strip(), "completion": output_text}
        return {"prompt": str(self.input_data), "completion": ""}

    def convert_to_text(self) -> Dict[str, str]:
        """è½¬æ¢ä¸ºtextæ ¼å¼"""
        if isinstance(self.input_data, dict):
            # æå–å­—æ®µ
            input_text = next((self.input_data[f] for f in self.INPUT_FIELDS if f in self.input_data), "")
            context_text = next((self.input_data[f] for f in self.CONTEXT_FIELDS if f in self.input_data), "")
            output_text = next((self.input_data[f] for f in self.OUTPUT_FIELDS if f in self.input_data), "")
            
            # æ„é€ å®Œæ•´æ–‡æœ¬
            text = ""
            if context_text:
                text += f"{context_text}\n\n"
            if input_text:
                text += f"{input_text}\n\n"
            text += output_text
        else:
            text = str(self.input_data)
        return {"text": text}

    def convert(self) -> Dict[str, Any]:
        """ä¸»è½¬æ¢å‡½æ•°"""
        format_to_use = self.target_format or self.detect_format()
        if format_to_use == "chat":
            return self.convert_to_chat()
        elif format_to_use == "completions":
            return self.convert_to_completions()
        else:
            return self.convert_to_text()

def convert_dataset():
    """æ•°æ®é›†è½¬æ¢åŠŸèƒ½"""
    console.print("\n[bold cyan]æ•°æ®é›†è½¬æ¢[/bold cyan]")
    
    # æ£€æŸ¥æ•°æ®é›†ç›®å½•
    datasets = []
    try:
        for item in os.listdir(BASE_DATASET_DIR):
            if (os.path.isdir(os.path.join(BASE_DATASET_DIR, item)) 
                and not item.startswith('MLX_') 
                and not item.startswith('.')):  # è¿‡æ»¤æ‰éšè—ç›®å½•
                datasets.append(item)
    except FileNotFoundError:
        console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®é›†ç›®å½•[/red]")
        return
    
    if not datasets:
        console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ•°æ®é›†ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®é›†[/yellow]")
        return
    
    # æ˜¾ç¤ºå¯ç”¨æ•°æ®é›†åˆ—è¡¨
    console.print("\n[bold]å¯ç”¨æ•°æ®é›†åˆ—è¡¨:[/bold]")
    for i, dataset in enumerate(datasets, 1):
        console.print(f"[{i}] {dataset}")
    
    # é€‰æ‹©æ•°æ®é›†
    try:
        dataset_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©æ•°æ®é›†",
            choices=[str(i) for i in range(1, len(datasets) + 1)]
        ))
    except ValueError:
        console.print("[red]æ— æ•ˆçš„é€‰æ‹©[/red]")
        return

    selected_dataset = datasets[dataset_choice - 1]
    source_dir = os.path.join(BASE_DATASET_DIR, selected_dataset)
    
    # é€‰æ‹©ç›®æ ‡æ ¼å¼
    console.print("\n[bold]è¯·é€‰æ‹©è½¬æ¢æ ¼å¼:[/bold]")
    console.print("[1] Completions æ ¼å¼")
    console.print("[2] Chat æ ¼å¼")
    console.print("[3] Text æ ¼å¼")
    
    try:
        format_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©æ ¼å¼",
            choices=["1", "2", "3"]
        ))
    except ValueError:
        console.print("[red]æ— æ•ˆçš„é€‰æ‹©[/red]")
        return
    
    # æ˜ å°„é€‰æ‹©åˆ°æ ¼å¼
    format_map = {
        1: "completions",
        2: "chat",
        3: "text"
    }
    target_format = format_map[format_choice]
    
    # å¦‚æœé€‰æ‹©äº†completionsæˆ–chatæ ¼å¼ï¼Œè¯·æ±‚system message
    system_message = None
    if target_format in ["completions", "chat"]:
        console.print("\n[bold]è¯·è¾“å…¥ç³»ç»Ÿæç¤ºè¯ï¼š[/bold]")
        system_message = Prompt.ask("ç³»ç»Ÿæç¤ºè¯", show_default=False)
        if not system_message.strip():
            system_message = None
    
    # æ ¹æ®é€‰æ‹©çš„æ ¼å¼è®¾ç½®ç›®æ ‡ç›®å½•åç§°
    format_prefix = target_format.capitalize()
    target_dir = os.path.join(BASE_DATASET_DIR, f"MLX_{format_prefix}_{selected_dataset}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ JSON/JSONL æ–‡ä»¶
    data_files = []
    file_sizes = {}
    for root, _, files in os.walk(source_dir):
        for file in files:
            if file.endswith(('.json', '.jsonl')):
                file_path = os.path.join(root, file)
                try:
                    file_size = os.path.getsize(file_path)
                    data_files.append(file_path)
                    file_sizes[file_path] = file_size
                except OSError as e:
                    console.print(f"[yellow]è­¦å‘Š: æ— æ³•è·å–æ–‡ä»¶å¤§å° {file}: {str(e)}[/yellow]")
    
    if not data_files:
        console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶[/red]")
        return
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åº
    sorted_files = sorted(data_files, key=lambda x: file_sizes[x], reverse=True)
    
    # è½¬æ¢æ‰€æœ‰éJSONLæ–‡ä»¶
    jsonl_files = []
    for file_path in sorted_files:
        if not file_path.endswith('.jsonl'):
            console.print(f"\n[yellow]æ­£åœ¨å°† {os.path.basename(file_path)} è½¬æ¢ä¸ºJSONLæ ¼å¼...[/yellow]")
            try:
                jsonl_path = convert_to_jsonl(file_path)
                jsonl_files.append(jsonl_path)
            except Exception as e:
                console.print(f"[red]è½¬æ¢å¤±è´¥: {str(e)}[/red]")
                continue
        else:
            jsonl_files.append(file_path)
    
    if not jsonl_files:
        console.print("[red]é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„JSONLæ–‡ä»¶[/red]")
        return
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    os.makedirs(target_dir, exist_ok=True)
    
    # è½¬æ¢æ•°æ®é›†
    for source_file in jsonl_files:
        target_file = os.path.join(target_dir, os.path.basename(source_file))
        console.print(f"\n[yellow]æ­£åœ¨è½¬æ¢: {os.path.basename(source_file)} -> {format_prefix} æ ¼å¼[/yellow]")
        
        try:
            with open(source_file, 'r', encoding='utf-8') as f_in, \
                 open(target_file, 'w', encoding='utf-8') as f_out:
                for line in f_in:
                    data = json.loads(line.strip())
                    converter = MLXDataConverter(data, target_format, system_message)
                    converted_data = converter.convert()
                    f_out.write(json.dumps(converted_data, ensure_ascii=False) + '\n')
            
            console.print(f"[green]âœ“ å·²è½¬æ¢å¹¶ä¿å­˜åˆ°: {target_file}[/green]")
        except Exception as e:
            console.print(f"[red]è½¬æ¢å¤±è´¥: {str(e)}[/red]")
            continue
    
    console.print(f"\n[green]æ•°æ®é›†å·²è½¬æ¢ä¸º {format_prefix} æ ¼å¼![/green]")
    console.print(f"[green]è½¬æ¢åçš„æ•°æ®é›†ä¿å­˜åœ¨: {target_dir}[/green]")

def split_dataset():
    """æ•°æ®é›†åˆ†å‰²åŠŸèƒ½"""
    console.print("\n[bold cyan]æ•°æ®é›†åˆ†å‰²[/bold cyan]")
    
    # æ£€æŸ¥æ•°æ®é›†ç›®å½•
    datasets = []
    try:
        for item in os.listdir(BASE_DATASET_DIR):
            if (os.path.isdir(os.path.join(BASE_DATASET_DIR, item)) 
                and item.startswith('MLX_')):  # åªæ˜¾ç¤ºMLX_å¼€å¤´çš„æ•°æ®é›†
                datasets.append(item)
    except FileNotFoundError:
        console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®é›†ç›®å½•[/red]")
        return
    
    if not datasets:
        console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„è½¬æ¢åæ•°æ®é›†ï¼Œè¯·å…ˆè¿›è¡Œæ•°æ®é›†è½¬æ¢[/yellow]")
        return
    
    # æ˜¾ç¤ºå¯ç”¨æ•°æ®é›†åˆ—è¡¨
    console.print("\n[bold]å¯ç”¨æ•°æ®é›†åˆ—è¡¨:[/bold]")
    for i, dataset in enumerate(datasets, 1):
        console.print(f"[{i}] {dataset}")
    
    # é€‰æ‹©æ•°æ®é›†
    try:
        dataset_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©æ•°æ®é›†",
            choices=[str(i) for i in range(1, len(datasets) + 1)]
        ))
    except ValueError:
        console.print("[red]æ— æ•ˆçš„é€‰æ‹©[/red]")
        return

    selected_dataset = datasets[dataset_choice - 1]
    dataset_dir = os.path.join(BASE_DATASET_DIR, selected_dataset)
    
    # æŸ¥æ‰¾JSONLæ–‡ä»¶
    jsonl_files = []
    for root, _, files in os.walk(dataset_dir):
        for file in files:
            if file.endswith('.jsonl'):
                jsonl_files.append(os.path.join(root, file))
    
    if not jsonl_files:
        console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°JSONLæ–‡ä»¶[/red]")
        return
    
    # è¯»å–æ•°æ®
    data = []
    for file in jsonl_files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line.strip()))
        except Exception as e:
            console.print(f"[red]è¯»å–æ–‡ä»¶ {file} æ—¶å‡ºé”™: {str(e)}[/red]")
            return
    
    total_samples = len(data)
    if total_samples == 0:
        console.print("[red]é”™è¯¯: æ•°æ®é›†ä¸ºç©º[/red]")
        return
    
    # è¯¢é—®éšæœºæŠ½å–æ•°é‡
    console.print(f"\n[bold]æ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples}[/bold]")
    try:
        sample_size = int(Prompt.ask(
            "è¯·è¾“å…¥éœ€è¦éšæœºæŠ½å–çš„æ ·æœ¬æ•°ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰",
            default=str(total_samples)
        ))
        if not 0 < sample_size <= total_samples:
            raise ValueError("æ ·æœ¬æ•°è¶…å‡ºèŒƒå›´")
    except ValueError as e:
        console.print(f"[red]æ— æ•ˆçš„æ ·æœ¬æ•°: {str(e)}[/red]")
        return
    
    # è¯¢é—®è®­ç»ƒé›†æ¯”ä¾‹
    try:
        train_ratio = float(Prompt.ask(
            "è¯·è¾“å…¥è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼Œé»˜è®¤0.9ï¼‰",
            default="0.9"
        ))
        if not 0 < train_ratio <= 1:
            raise ValueError("æ¯”ä¾‹å¿…é¡»åœ¨0-1ä¹‹é—´")
    except ValueError as e:
        console.print(f"[red]æ— æ•ˆçš„æ¯”ä¾‹: {str(e)}[/red]")
        return
    
    # è¯¢é—®æµ‹è¯•é›†æ¯”ä¾‹
    if train_ratio < 1:
        try:
            test_ratio = float(Prompt.ask(
                "è¯·è¾“å…¥æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼Œé»˜è®¤0.5ï¼‰",
                default="0.5"
            ))
            if not 0 < test_ratio <= 1:
                raise ValueError("æ¯”ä¾‹å¿…é¡»åœ¨0-1ä¹‹é—´")
        except ValueError as e:
            console.print(f"[red]æ— æ•ˆçš„æ¯”ä¾‹: {str(e)}[/red]")
            return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "/Users/wyek1n/Downloads/Code/MLX/MLX-CLI/lora/data"
    os.makedirs(output_dir, exist_ok=True)
    
    # éšæœºæŠ½æ ·å’Œåˆ†å‰²
    import random
    random.shuffle(data)
    selected_data = data[:sample_size]
    
    # è®¡ç®—å„é›†åˆçš„å¤§å°
    train_size = int(sample_size * train_ratio)
    if train_ratio < 1:
        remaining_size = sample_size - train_size
        test_size = int(remaining_size * test_ratio)
        valid_size = remaining_size - test_size
    else:
        test_size = valid_size = 0
    
    # åˆ†å‰²æ•°æ®
    train_data = selected_data[:train_size]
    if train_ratio < 1:
        test_data = selected_data[train_size:train_size + test_size]
        valid_data = selected_data[train_size + test_size:]
    
    # ä¿å­˜æ•°æ®
    def save_jsonl(data, filename):
        filepath = os.path.join(output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        return filepath
    
    # ä¿å­˜è®­ç»ƒé›†
    train_file = save_jsonl(train_data, 'train.jsonl')
    console.print(f"\n[green]å·²ä¿å­˜è®­ç»ƒé›† ({len(train_data)} æ¡æ•°æ®): {train_file}[/green]")
    
    # ä¿å­˜æµ‹è¯•é›†å’ŒéªŒè¯é›†
    if train_ratio < 1:
        test_file = save_jsonl(test_data, 'test.jsonl')
        valid_file = save_jsonl(valid_data, 'valid.jsonl')
        console.print(f"[green]å·²ä¿å­˜æµ‹è¯•é›† ({len(test_data)} æ¡æ•°æ®): {test_file}[/green]")
        console.print(f"[green]å·²ä¿å­˜éªŒè¯é›† ({len(valid_data)} æ¡æ•°æ®): {valid_file}[/green]")

def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
            
        # è®¾ç½®é»˜è®¤å€¼
        defaults = {
            "train": True,
            "seed": 0,
            "num_layers": 32,
            "batch_size": 1,
            "iters": 100,
            "val_batches": 25,
            "learning_rate": 1e-6,
            "steps_per_report": 10,
            "steps_per_eval": 200,
            "resume_adapter_file": None,
            "save_every": 1000,
            "test": False,
            "test_batches": 100,
            "max_seq_length": 8192,
            "grad_checkpoint": True,
            "fine_tune_type": "lora"
        }
        
        # æ›´æ–°é»˜è®¤å€¼
        for key, value in defaults.items():
            if key not in config:
                config[key] = value
                
        return config
    except Exception as e:
        console.print(f"[red]åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}[/red]")
        return None

def get_model_layers(model_path: str) -> int:
    """è·å–æ¨¡å‹çš„å®é™…å±‚æ•°"""
    try:
        from mlx_lm import load
        model, _ = load(model_path)
        return len(model.layers)
    except Exception as e:
        console.print(f"[yellow]è­¦å‘Š: æ— æ³•è·å–æ¨¡å‹å±‚æ•°: {str(e)}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼[/yellow]")
        return 24  # Qwen2.5-0.5B çš„é»˜è®¤å±‚æ•°

def send_telegram_message(message: str, photo_path: str = None):
    """å‘é€ Telegram æ¶ˆæ¯"""
    bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHAT_ID")
    
    if not bot_token or not chat_id:
        console.print("[yellow]è­¦å‘Š: æœªæ‰¾åˆ° Telegram é…ç½®ï¼Œå°†ä¸ä¼šå‘é€é€šçŸ¥[/yellow]")
        return
    
    url = f"https://api.telegram.org/bot{bot_token}/"
    
    try:
        if photo_path and os.path.exists(photo_path):
            # å‘é€å›¾ç‰‡
            with open(photo_path, 'rb') as photo:
                response = requests.post(
                    url + "sendPhoto",
                    data={
                        "chat_id": chat_id,
                        "caption": message,
                        "parse_mode": "HTML"
                    },
                    files={"photo": photo}
                )
            if response.status_code != 200:
                log.error(f"å‘é€å›¾ç‰‡åˆ° Telegram å¤±è´¥: {response.text}")
            else:
                log.debug("æˆåŠŸå‘é€å›¾ç‰‡åˆ° Telegram")
        else:
            # åªå‘é€æ–‡æœ¬æ¶ˆæ¯
            response = requests.post(
                url + "sendMessage",
                json={
                    "chat_id": chat_id,
                    "text": message,
                    "parse_mode": "HTML"
                }
            )
            if response.status_code != 200:
                log.error(f"å‘é€æ¶ˆæ¯åˆ° Telegram å¤±è´¥: {response.text}")
            
        response.raise_for_status()
    except Exception as e:
        console.print(f"[yellow]å‘é€ Telegram é€šçŸ¥å¤±è´¥: {str(e)}[/yellow]")
        log.error(f"å‘é€ Telegram é€šçŸ¥å¤±è´¥: {str(e)}")

def format_time_duration(seconds: int) -> str:
    """æ ¼å¼åŒ–æ—¶é—´é—´éš”"""
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    seconds = seconds % 60
    
    parts = []
    if hours > 0:
        parts.append(f"{hours}å°æ—¶")
    if minutes > 0:
        parts.append(f"{minutes}åˆ†é’Ÿ")
    if seconds > 0 or not parts:
        parts.append(f"{seconds}ç§’")
    
    return "".join(parts)

def fine_tune():
    """æ¨¡å‹å¾®è°ƒåŠŸèƒ½"""
    start_time = time.time()
    
    console.print("\n[bold cyan]æ¨¡å‹å¾®è°ƒ[/bold cyan]")
    
    # æ£€æŸ¥ wandb API key
    wandb_api_key = os.getenv("WANDB_API_KEY")
    if not wandb_api_key:
        console.print("[yellow]è­¦å‘Š: æœªæ‰¾åˆ°WANDB_API_KEYï¼Œå°†ä¸ä¼šè®°å½•è®­ç»ƒè¿‡ç¨‹[/yellow]")
    
    # æ£€æŸ¥å¹¶é€‰æ‹©æ¨¡å‹
    models = []
    try:
        for item in os.listdir(BASE_MODEL_DIR):
            if os.path.isdir(os.path.join(BASE_MODEL_DIR, item)):
                if not item.startswith('.'):
                    models.append(item)
    except FileNotFoundError:
        console.print("[red]é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•[/red]")
        return
    
    if not models:
        console.print("[yellow]æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹[/yellow]")
        return
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
    console.print("\n[bold]å¯ç”¨æ¨¡å‹åˆ—è¡¨:[/bold]")
    for i, model in enumerate(models, 1):
        console.print(f"[{i}] {model}")
    console.print("[0] è¿”å›ä¸»èœå•")
    
    # é€‰æ‹©æ¨¡å‹
    try:
        model_choice = int(Prompt.ask(
            "è¯·é€‰æ‹©æ¨¡å‹",
            choices=["0"] + [str(i) for i in range(1, len(models) + 1)]
        ))
        
        if model_choice == 0:
            return
            
        selected_model = models[model_choice - 1]
        model_path = os.path.join(BASE_MODEL_DIR, selected_model)
        
        # è·å–æ¨¡å‹å®é™…å±‚æ•°
        model_layers = get_model_layers(model_path)
        console.print(f"[cyan]æ¨¡å‹å±‚æ•°: {model_layers}[/cyan]")
        
        # åˆ›å»º adapter æƒé‡ä¿å­˜ç›®å½•
        adapter_path = os.path.join(ADAPTER_DIR, selected_model)  # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„è·¯å¾„
        os.makedirs(adapter_path, exist_ok=True)
        
        # è¯¢é—®æ˜¯å¦ä½¿ç”¨é…ç½®æ–‡ä»¶
        use_config = Prompt.ask("æ˜¯å¦ä½¿ç”¨é…ç½®æ–‡ä»¶?", choices=["y", "n"], default="n").lower() == "y"
        
        if use_config:
            config_path = "config.yaml"
            if not os.path.exists(config_path):
                console.print("[yellow]é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶[/yellow]")
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹è·¯å¾„
                default_config = {
                    "model": model_path,
                    "train": True,
                    "seed": 0,
                    "num_layers": model_layers,  # ä½¿ç”¨å®é™…çš„å±‚æ•°
                    "batch_size": 1,
                    "iters": 100,
                    "val_batches": 25,
                    "learning_rate": 1e-6,
                    "steps_per_report": 10,
                    "steps_per_eval": 200,
                    "resume_adapter_file": None,
                    "save_every": 1000,
                    "test": False,
                    "test_batches": 100,
                    "max_seq_length": 8192,
                    "grad_checkpoint": True,
                    "fine_tune_type": "lora",
                    "adapter_path": adapter_path,  # æ·»åŠ  adapter ä¿å­˜è·¯å¾„
                    "data_path": "/Users/wyek1n/Downloads/Code/MLX/MLX-CLI/lora/data"  # æ·»åŠ æ•°æ®é›†è·¯å¾„
                }
                with open(config_path, "w", encoding="utf-8") as f:
                    yaml.dump(default_config, f, allow_unicode=True)
                console.print("[green]å·²åˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œè¯·æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®åé‡æ–°è¿è¡Œ[/green]")
                return
            else:
                # åŠ è½½ç°æœ‰é…ç½®æ–‡ä»¶å¹¶æ›´æ–°æ¨¡å‹å’Œæ•°æ®é›†è·¯å¾„
                config = load_config(config_path)
                if not config:
                    return
                
                # æ›´æ–°é…ç½®
                config["model"] = model_path
                config["adapter_path"] = adapter_path
                config["data_path"] = "/Users/wyek1n/Downloads/Code/MLX/MLX-CLI/lora/data"
                
                # ä¿å­˜æ›´æ–°åçš„é…ç½®
                with open(config_path, "w", encoding="utf-8") as f:
                    yaml.dump(config, f, allow_unicode=True)
                
                console.print("[green]å·²æ›´æ–°é…ç½®æ–‡ä»¶[/green]")
                params = config
        else:
            # äº¤äº’å¼è®¾ç½®å‚æ•°
            params = {
                "batch_size": IntPrompt.ask("è¯·è¾“å…¥æ‰¹æ¬¡å¤§å°", default=1),
                "num_layers": IntPrompt.ask(
                    "è¯·è¾“å…¥å¾®è°ƒå±‚æ•°",
                    default=model_layers,
                    show_choices=False,
                    show_default=True
                ),
                "iters": IntPrompt.ask("è¯·è¾“å…¥è®­ç»ƒè¿­ä»£æ¬¡æ•°", default=100),
                "learning_rate": float(Prompt.ask("è¯·è¾“å…¥å­¦ä¹ ç‡", default="1e-6")),
                "val_batches": IntPrompt.ask("è¯·è¾“å…¥éªŒè¯æ‰¹æ¬¡æ•°", default=25),
                "steps_per_eval": IntPrompt.ask("è¯·è¾“å…¥éªŒè¯é—´éš”æ­¥æ•°", default=200),
                "save_every": IntPrompt.ask("è¯·è¾“å…¥ä¿å­˜é—´éš”æ­¥æ•°", default=1000),
                "max_seq_length": IntPrompt.ask("è¯·è¾“å…¥æœ€å¤§åºåˆ—é•¿åº¦", default=8192),
                "fine_tune_type": Prompt.ask("è¯·é€‰æ‹©å¾®è°ƒç±»å‹", choices=["lora", "dora", "full"], default="lora"),
                "grad_checkpoint": Prompt.ask("æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹? [y/n]", choices=["y", "n"], default="n").lower() == "y"
            }
            
            # éªŒè¯å±‚æ•°
            if params["num_layers"] > model_layers:
                console.print(f"[yellow]è­¦å‘Š: è®¾ç½®çš„å±‚æ•° ({params['num_layers']}) è¶…è¿‡æ¨¡å‹å®é™…å±‚æ•° ({model_layers})ï¼Œå°†ä½¿ç”¨å®é™…å±‚æ•°[/yellow]")
                params["num_layers"] = model_layers
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤ï¼ˆç§»åˆ°è¿™é‡Œï¼‰
        cmd = [
            "python", "-m",
            "mlx_lm.lora",
            "--train",
            "--model", model_path,
            "--adapter-path", adapter_path,
            "--fine-tune-type", params["fine_tune_type"],
            "--num-layers", str(params["num_layers"]),
            "--batch-size", str(params["batch_size"]),
            "--iters", str(params["iters"]),
            "--val-batches", str(params["val_batches"]),
            "--learning-rate", str(params["learning_rate"]),
            "--steps-per-eval", str(params["steps_per_eval"]),
            "--save-every", str(params["save_every"]),
            "--max-seq-length", str(params["max_seq_length"]),
            "--data", "/Users/wyek1n/Downloads/Code/MLX/MLX-CLI/lora/data"
        ]
        
        if params["grad_checkpoint"]:
            cmd.append("--grad-checkpoint")
        
        # æ˜¾ç¤ºå‘½ä»¤é¢„è§ˆ
        console.print("\n[bold]å°†æ‰§è¡Œä»¥ä¸‹å‘½ä»¤:[/bold]")
        console.print(" ".join(cmd))
        
        # ç¡®è®¤æ‰§è¡Œ
        if not Prompt.ask("\næ˜¯å¦å¼€å§‹è®­ç»ƒ?", choices=["y", "n"], default="y").lower() == "y":
            console.print("[yellow]å·²å–æ¶ˆè®­ç»ƒ[/yellow]")
            return
        
        # å‘é€å¼€å§‹è®­ç»ƒé€šçŸ¥
        start_message = (
            f"ğŸš€ <b>æ¨¡å‹å¾®è°ƒå¼€å§‹</b>\n\n"
            f"æ¨¡å‹: {selected_model}\n\n"
            f"æ‰¹æ¬¡å¤§å°: {params['batch_size']}\n"
            f"å¾®è°ƒå±‚æ•°: {params['num_layers']}\n"
            f"è®­ç»ƒè¿­ä»£: {params['iters']}\n"
            f"å­¦ä¹ ç‡: {params['learning_rate']}\n"
            f"å¾®è°ƒç±»å‹: {params['fine_tune_type']}\n"
            f"éªŒè¯æ‰¹æ¬¡: {params['val_batches']}\n"
            f"éªŒè¯é—´éš”: {params['steps_per_eval']}\n"
            f"ä¿å­˜é—´éš”: {params['save_every']}\n"
            f"æœ€å¤§é•¿åº¦: {params['max_seq_length']}\n"
            f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {'æ˜¯' if params['grad_checkpoint'] else 'å¦'}"
        )
        send_telegram_message(start_message)
        
        try:
            # åˆå§‹åŒ– wandbï¼ˆå¦‚æœæœ‰API keyï¼‰
            if wandb_api_key:
                try:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    run_name = f"{selected_model}_{timestamp}"
                    
                    wandb.init(
                        project="mlx-finetune",
                        name=run_name,
                        config={
                            "model": selected_model,
                            "batch_size": params["batch_size"],
                            "num_layers": params["num_layers"],
                            "iters": params["iters"],
                            "learning_rate": params["learning_rate"]
                        }
                    )
                    
                    # å®šä¹‰è¦è¿½è¸ªçš„æŒ‡æ ‡
                    wandb.define_metric("train/global_step", summary="max")
                    wandb.define_metric("train/epoch", summary="max")
                    wandb.define_metric("train/loss", summary="min")
                    wandb.define_metric("train/learning_rate", summary="last")
                    wandb.define_metric("performance/iterations_per_second", summary="mean")
                    wandb.define_metric("performance/tokens_per_second", summary="mean")
                    wandb.define_metric("performance/total_tokens", summary="max")
                    wandb.define_metric("performance/peak_memory_gb", summary="max")
                    
                    console.print(f"[green]wandb run åˆå§‹åŒ–æˆåŠŸ: {run_name}[/green]")
                except Exception as e:
                    console.print(f"[yellow]wandb åˆå§‹åŒ–å¤±è´¥: {str(e)}ï¼Œå°†ä¸ä¼šè®°å½•è®­ç»ƒè¿‡ç¨‹[/yellow]")
                    wandb_api_key = None
            
            # æ‰§è¡Œè®­ç»ƒ
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,  # åˆå¹¶æ ‡å‡†é”™è¯¯åˆ°æ ‡å‡†è¾“å‡º
                universal_newlines=True,
                bufsize=1,
                preexec_fn=os.setsid,  # åœ¨æ–°çš„è¿›ç¨‹ç»„ä¸­è¿è¡Œ
                env=os.environ.copy()  # ä½¿ç”¨å½“å‰ç¯å¢ƒå˜é‡
            )
            
            def handle_signal(signum, frame):
                """å¤„ç†ä¸­æ–­ä¿¡å·"""
                if process.poll() is None:  # å¦‚æœè¿›ç¨‹è¿˜åœ¨è¿è¡Œ
                    try:
                        # ç»ˆæ­¢æ•´ä¸ªè¿›ç¨‹ç»„
                        os.killpg(os.getpgid(process.pid), signal.SIGTERM)
                        process.wait(timeout=5)  # ç­‰å¾…è¿›ç¨‹ç»“æŸ
                    except:
                        # å¦‚æœè¿›ç¨‹æ²¡æœ‰åŠæ—¶ç»“æŸï¼Œå¼ºåˆ¶ç»ˆæ­¢
                        try:
                            os.killpg(os.getpgid(process.pid), signal.SIGKILL)
                        except:
                            pass
                raise KeyboardInterrupt
            
            # è®¾ç½®ä¿¡å·å¤„ç†å™¨
            original_sigint = signal.getsignal(signal.SIGINT)
            original_sigtstp = signal.getsignal(signal.SIGTSTP)
            signal.signal(signal.SIGINT, handle_signal)
            signal.signal(signal.SIGTSTP, handle_signal)
            
            try:
                # å®æ—¶æ˜¾ç¤ºè¾“å‡ºå¹¶è®°å½•åˆ° wandb
                while process.poll() is None:
                    line = process.stdout.readline()
                    if line:
                        line = line.strip()
                        print(line, flush=True)
                        
                        # è§£æè®­ç»ƒæŒ‡æ ‡å¹¶è®°å½•åˆ° wandb
                        if wandb_api_key and ("Train loss" in line or "Val loss" in line):
                            try:
                                # è§£æè®­ç»ƒè¾“å‡º
                                metrics = {}
                                
                                # è§£æè¿­ä»£ä¿¡æ¯
                                if "Iter" in line:
                                    iter_match = re.search(r'Iter\s*(\d+)', line)
                                    if iter_match:
                                        current_iter = int(iter_match.group(1))
                                        metrics["train/global_step"] = current_iter
                                        metrics["train/epoch"] = current_iter / params["iters"]
                                
                                # è§£æè®­ç»ƒæŸå¤±
                                if "Train loss" in line:
                                    loss_match = re.search(r'Train loss\s*([\d.]+)', line)
                                    if loss_match:
                                        metrics["train/loss"] = float(loss_match.group(1))
                                
                                # è§£æéªŒè¯æŸå¤±
                                if "Val loss" in line:
                                    val_loss_match = re.search(r'Val loss\s*([\d.]+)', line)
                                    if val_loss_match:
                                        metrics["val/loss"] = float(val_loss_match.group(1))
                                
                                # è§£æå­¦ä¹ ç‡
                                if "Learning Rate" in line:
                                    lr_match = re.search(r'Learning Rate\s*([\d.e-]+)', line)
                                    if lr_match:
                                        metrics["train/learning_rate"] = float(lr_match.group(1))
                                
                                # è§£ææ€§èƒ½æŒ‡æ ‡
                                if "It/sec" in line:
                                    its_match = re.search(r'It/sec\s*([\d.]+)', line)
                                    if its_match:
                                        metrics["performance/iterations_per_second"] = float(its_match.group(1))
                                
                                if "Tokens/sec" in line:
                                    tps_match = re.search(r'Tokens/sec\s*([\d.]+)', line)
                                    if tps_match:
                                        metrics["performance/tokens_per_second"] = float(tps_match.group(1))
                                
                                if "Trained Tokens" in line:
                                    tokens_match = re.search(r'Trained Tokens\s*(\d+)', line)
                                    if tokens_match:
                                        metrics["performance/total_tokens"] = int(tokens_match.group(1))
                                
                                if "Peak mem" in line:
                                    mem_match = re.search(r'Peak mem\s*([\d.]+)', line)
                                    if mem_match:
                                        metrics["performance/peak_memory_gb"] = float(mem_match.group(1))
                                
                                if metrics:
                                    wandb.log(metrics)
                                    log.debug(f"è®°å½•æŒ‡æ ‡: {metrics}")
                            except Exception as e:
                                log.error(f"è®°å½•æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
                
                # è¯»å–å‰©ä½™è¾“å‡º
                remaining_output = process.stdout.read()
                if remaining_output:
                    print(remaining_output.strip(), flush=True)
                
                if process.returncode != 0:
                    raise subprocess.CalledProcessError(process.returncode, cmd)
                        
            finally:
                # æ¢å¤åŸå§‹ä¿¡å·å¤„ç†å™¨
                signal.signal(signal.SIGINT, original_sigint)
                signal.signal(signal.SIGTSTP, original_sigtstp)
                
                # ç¡®ä¿è¿›ç¨‹è¢«ç»ˆæ­¢
                if process.poll() is None:
                    try:
                        os.killpg(os.getpgid(process.pid), signal.SIGTERM)
                        process.wait(timeout=5)
                    except:
                        try:
                            os.killpg(os.getpgid(process.pid), signal.SIGKILL)
                        except:
                            pass
                
        except KeyboardInterrupt:
            end_message = "âŒ <b>è®­ç»ƒå·²è¢«ç”¨æˆ·ä¸­æ–­</b>"
            send_telegram_message(end_message)
            console.print("\n[yellow]è®­ç»ƒå·²è¢«ç”¨æˆ·ä¸­æ–­[/yellow]")
        except subprocess.CalledProcessError as e:
            end_message = f"âŒ <b>è®­ç»ƒè¿›ç¨‹å‡ºé”™</b>\n\né”™è¯¯ä¿¡æ¯: {str(e)}"
            send_telegram_message(end_message)
            console.print(f"[red]è®­ç»ƒè¿›ç¨‹å‡ºé”™: {str(e)}[/red]")
        except Exception as e:
            end_message = f"âŒ <b>æ‰§è¡Œå‡ºé”™</b>\n\né”™è¯¯ä¿¡æ¯: {str(e)}"
            send_telegram_message(end_message)
            console.print(f"[red]æ‰§è¡Œå‡ºé”™: {str(e)}[/red]")
        finally:
            # è®¡ç®—è®­ç»ƒæ—¶é—´
            end_time = time.time()
            duration = format_time_duration(int(end_time - start_time))
            
            if process.returncode == 0:
                # è·å– wandb è¿è¡Œçš„ URL
                wandb_url = wandb.run.get_url() if wandb.run else "æœªä½¿ç”¨ wandb"
                
                # è·å–æœ€ç»ˆçš„è®­ç»ƒæŒ‡æ ‡
                final_metrics = {}
                end_message = None  # åˆå§‹åŒ– end_message
                notification_sent = False  # æ·»åŠ é€šçŸ¥å‘é€æ ‡å¿—
                
                try:
                    if wandb.run:
                        # è·å–è®­ç»ƒå†å²
                        api = wandb.Api()
                        run = api.run(f"wyek1n-wye/mlx-finetune/{wandb.run.id}")
                        
                        # ç­‰å¾…åŒæ­¥å®Œæˆ
                        while not run.summary.get("_wandb", {}).get("runtime", 0):
                            time.sleep(1)
                        
                        # è·å–å†å²æ•°æ®
                        history = pd.DataFrame(run.scan_history())
                        
                        if len(history) > 0:  # æ£€æŸ¥æ˜¯å¦æœ‰å†å²æ•°æ®
                            final_metrics = {
                                "loss": history["train/loss"].iloc[-1],
                                "perplexity": math.exp(history["train/loss"].iloc[-1]),
                                "total_tokens": history["performance/total_tokens"].iloc[-1],
                                "tokens_per_second": history["performance/tokens_per_second"].mean(),
                                "peak_memory": history["performance/peak_memory_gb"].max()
                            }
                            
                            # å…ˆæ‰§è¡Œæ¨¡å‹è¯„ä¼°
                            test_cmd = [
                                "python", "-m", "mlx_lm.lora",
                                "--model", model_path,
                                "--data", "/Users/wyek1n/Downloads/Code/MLX/MLX-CLI/lora/data",
                                "--adapter-path", os.path.join("/Users/wyek1n/Downloads/MLX/adapter", selected_model),  # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºé€‚é…å™¨ç›®å½•
                                "--test"
                            ]
                            
                            try:
                                test_output = subprocess.check_output(test_cmd, universal_newlines=True)
                                test_match = re.search(r'Test loss ([0-9.]+),\s*Test ppl ([0-9.]+)', test_output)
                                if test_match:
                                    test_loss = float(test_match.group(1))
                                    test_ppl = float(test_match.group(2).rstrip('.'))
                                    
                                    # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                                    console.print(f"\n[green]è¯„ä¼°ç»“æœ:[/green]")
                                    console.print(f"æ¨¡å‹: {selected_model}")
                                    console.print(f"é€‚é…å™¨: {selected_model}")  # ä½¿ç”¨ç›¸åŒçš„åç§°
                                    console.print(f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}")
                                    console.print(f"æµ‹è¯•é›†å›°æƒ‘åº¦: {test_ppl:.4f}")
                                    
                                    # æ›´æ–°é€šçŸ¥æ¶ˆæ¯ï¼Œæ·»åŠ è¯„ä¼°ç»“æœ
                                    end_message = (
                                        f"âœ… <b>æ¨¡å‹å¾®è°ƒå®Œæˆ</b>\n\n"
                                        f"æ¨¡å‹: {selected_model}\n"
                                        f"è®­ç»ƒæ—¶é•¿: {duration}\n"
                                        f"æœ€ç»ˆæŸå¤±: {final_metrics['loss']:.4f}\n"
                                        f"å›°æƒ‘åº¦: {final_metrics['perplexity']:.4f}\n"
                                        f"æ€»å¤„ç†tokens: {final_metrics['total_tokens']:,}\n"
                                        f"å¹³å‡é€Ÿåº¦: {final_metrics['tokens_per_second']:.2f} tokens/s\n"
                                        f"å³°å€¼å†…å­˜: {final_metrics['peak_memory']:.2f} GB\n\n"
                                        f"è¯„ä¼°ç»“æœ:\n"
                                        f"æ¨¡å‹: {selected_model}\n"
                                        f"é€‚é…å™¨: {selected_model}\n"  # ä½¿ç”¨ç›¸åŒçš„åç§°
                                        f"æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}\n"
                                        f"æµ‹è¯•é›†å›°æƒ‘åº¦: {test_ppl:.4f}\n\n"
                                        f"Wandb åœ°å€: {wandb_url}"
                                    )
                                    
                                    # è®¾ç½® matplotlib ä½¿ç”¨éäº¤äº’å¼åç«¯
                                    plt.switch_backend('Agg')
                                    
                                    # åˆ›å»ºå›¾è¡¨
                                    plt.figure(figsize=(12, 8))
                                    plt.rcParams.update({
                                        'font.size': 12,
                                        'axes.titlesize': 16,
                                        'axes.labelsize': 12,
                                        'axes.spines.top': False,
                                        'axes.spines.right': False,
                                        'axes.grid': True,
                                        'grid.alpha': 0.3,
                                        'grid.color': '#b0b0b0'
                                    })
                                    
                                    # ç»˜åˆ¶è®­ç»ƒæŸå¤±
                                    train_loss = history["train/loss"].values
                                    iterations = np.arange(len(train_loss)) * 10
                                    window_size = 10
                                    smoothed_train_loss = pd.Series(train_loss).rolling(window=window_size, min_periods=1, center=True).mean()
                                    plt.plot(iterations, smoothed_train_loss, color='#1f77b4', linewidth=2, label='Training Loss')
                                    
                                    # ç»˜åˆ¶éªŒè¯æŸå¤±
                                    if "val/loss" in history.columns:
                                        val_loss = history["val/loss"].values
                                        val_indices = history.index[history["val/loss"].notna()].values * 10
                                        val_loss_clean = val_loss[~np.isnan(val_loss)]
                                        plt.plot(val_indices, val_loss_clean, color='#ff7f0e', linewidth=2, label='Validation Loss')
                                    
                                    plt.title('Training and Validation Loss', pad=20, fontweight='bold')
                                    plt.xlabel('Iteration')
                                    plt.ylabel('Loss')
                                    plt.legend(frameon=False, loc='upper right', fontsize=12)
                                    plt.tight_layout()
                                    
                                    # ä¿å­˜å›¾è¡¨
                                    script_dir = os.path.dirname(os.path.abspath(__file__))
                                    loss_plot_path = os.path.join(script_dir, "loss_plot.png")
                                    plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
                                    plt.close()
                                    
                                    # å‘é€é€šçŸ¥å’Œå›¾ç‰‡
                                    if os.path.exists(loss_plot_path):
                                        send_telegram_message(end_message)
                                        send_telegram_message("", loss_plot_path)
                                        notification_sent = True  # æ ‡è®°é€šçŸ¥å·²å‘é€
                                        os.remove(loss_plot_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                else:
                                    console.print("[red]æ— æ³•è§£æè¯„ä¼°ç»“æœ[/red]")
                                    console.print(f"åŸå§‹è¾“å‡º: {test_output}")
                            except Exception as e:
                                log.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
                                console.print(f"[red]è¯„ä¼°å¤±è´¥: {str(e)}[/red]")
                            else:
                                log.warning("æœªæ‰¾åˆ°è®­ç»ƒå†å²æ•°æ®")
                                end_message = (
                                    f"âœ… <b>æ¨¡å‹å¾®è°ƒå®Œæˆ</b>\n\n"
                                    f"æ¨¡å‹: {selected_model}\n"
                                    f"è®­ç»ƒæ—¶é•¿: {duration}\n"
                                    f"Wandb åœ°å€: {wandb_url}"
                                )
                            
                            # å®Œæˆåå†å…³é—­ wandb
                            wandb.finish()
                except Exception as e:
                    log.error(f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {str(e)}")
                    import traceback
                    log.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    final_metrics = {}
                    end_message = (
                        f"âœ… <b>æ¨¡å‹å¾®è°ƒå®Œæˆ</b>\n\n"
                        f"æ¨¡å‹: {selected_model}\n"
                        f"è®­ç»ƒæ—¶é•¿: {duration}\n"
                        f"Wandb åœ°å€: {wandb_url}"
                    )
                
                # å¦‚æœè¿˜æ²¡æœ‰å‘é€è¿‡é€šçŸ¥ï¼Œåˆ™åœ¨è¿™é‡Œå‘é€
                if not notification_sent:
                    send_telegram_message(end_message)
            
            # ç¡®ä¿å…³é—­ wandb
            if wandb.run is not None:
                wandb.finish()

        # æ·»åŠ è®­ç»ƒå¤±è´¥çš„æ£€æŸ¥
        if "nan" in test_output.lower():
            log.error("è®­ç»ƒè¿‡ç¨‹å‡ºç°æ•°å€¼ä¸ç¨³å®š")
            console.print("[red]è®­ç»ƒå¤±è´¥ï¼šå‡ºç°æ•°å€¼ä¸ç¨³å®š(NaN)ï¼Œè¯·å°è¯•é™ä½å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ•°æ®[/red]")
            return
            
    except Exception as e:
        console.print(f"[red]å‡ºç°é”™è¯¯: {str(e)}[/red]")
        return

@app.callback(invoke_without_command=True)
def main(ctx: typer.Context):
    """MLX CLI ä¸»ç¨‹åº"""
    if ctx.invoked_subcommand is None:
        # æ¸…ç†å±å¹•
        os.system('cls' if os.name == 'nt' else 'clear')
        
        show_header()
        check_env_vars()
        check_environment()
        
        # åˆå§‹åŒ–é…ç½®
        config = Config()
        global show_logs, rich_handler
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½æ—¥å¿—æ˜¾ç¤ºçŠ¶æ€
        show_logs = config.get("show_logs", False)
        rich_handler.enabled = show_logs
        
        while True:
            choice = show_menu()
            
            if choice == 0:
                console.print("[cyan]æ„Ÿè°¢ä½¿ç”¨,å†è§![/cyan]")
                break
            elif choice == 1:
                download_model()
            elif choice == 2:
                chat_with_model()
            elif choice == 3:
                prepare_data()
            elif choice == 4:
                fine_tune()
            elif choice == 5:
                merge_model()
            elif choice == 6:
                show_logs = not show_logs
                rich_handler.enabled = show_logs
                config.set("show_logs", show_logs)
                console.print(f"[green]å·²{'éšè—' if not show_logs else 'æ˜¾ç¤º'}æ—¥å¿—[/green]")

class Config:
    """é…ç½®ç®¡ç†ç±»"""
    def __init__(self):
        self.config_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "config.json")
        self.config = self.load_config()
    
    def load_config(self):
        """åŠ è½½é…ç½®"""
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                log.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {"show_logs": False}  # é»˜è®¤é…ç½®
    
    def save_config(self):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.config_file, 'w') as f:
                json.dump(self.config, f, indent=4)
        except Exception as e:
            log.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def get(self, key, default=None):
        """è·å–é…ç½®é¡¹"""
        return self.config.get(key, default)
    
    def set(self, key, value):
        """è®¾ç½®é…ç½®é¡¹"""
        self.config[key] = value
        self.save_config()

if __name__ == "__main__":
    app()
