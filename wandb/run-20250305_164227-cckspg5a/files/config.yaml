_wandb:
    value:
        cli_version: 0.19.7
        m:
            - "1": train/loss
              "6":
                - 3
              "7":
                - 1
            - "1": train/learning_rate
              "6":
                - 3
              "7":
                - 5
            - "1": train/global_step
              "6":
                - 3
              "7":
                - 2
            - "1": train/epoch
              "6":
                - 3
              "7":
                - 2
        python_version: 3.11.11
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 53
                - 55
            "2":
                - 1
                - 5
                - 11
                - 49
                - 53
                - 55
            "3":
                - 2
                - 7
                - 13
                - 16
                - 23
                - 55
                - 63
            "4": 3.11.11
            "5": 0.19.7
            "6": 4.49.0
            "8":
                - 5
            "12": 0.19.7
            "13": darwin-arm64
adapter_path:
    value: /Users/wyek1n/Downloads/MLX/adapter/Qwen2.5-0.5B-Instruct
batch_size:
    value: 1
fine_tune_type:
    value: lora
iters:
    value: 1000
learning_rate:
    value: 0.001
model:
    value: /Users/wyek1n/Downloads/MLX/model/Qwen2.5-0.5B-Instruct
num_layers:
    value: 16
steps_per_report:
    value: 10
